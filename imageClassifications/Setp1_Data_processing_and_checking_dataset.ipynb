{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Setp1. Data processing and checking dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing and checking dataset\n",
        "Assume you are going to establish a animals classifications CNN  \n",
        "To prepare dataset, your final data folder should be like this\n",
        "```\n",
        ".\n",
        "└── Dataset/\n",
        "    ├── Dog/\n",
        "    │   ├── dogImg1.jpg\n",
        "    │   ├── dogImg1.jpg\n",
        "    │   └── ...\n",
        "    ├── Cat/\n",
        "    │   ├── catImg1.jpg\n",
        "    │   ├── catImg1.jpg\n",
        "    │   └── ...\n",
        "    ├── Bird/\n",
        "    │   ├── birdImg1.jpg\n",
        "    │   ├── birdImg1.jpg\n",
        "    │   └── ...\n",
        "    ├── Hamster/\n",
        "    │   ├── hamsterImg1.jpg\n",
        "    │   ├── hamsterImg1.jpg\n",
        "    │   └── ...\n",
        "    └── ...\\\n",
        "```\n",
        "\n",
        "Each folder should contains same class images, and try to make all classes share the same / near numbers of images numbers.\n",
        "```\n",
        "** Good example **\n",
        "Dog (120 images)  \n",
        "Cat (131 images)\n",
        "Bird (125 images)\n",
        "Hamster (129 images)\n",
        "...\n",
        "\n",
        "** Bad example **\n",
        "Dog (120 images)  \n",
        "Cat (40 images)\n",
        "Bird (300 images)\n",
        "Hamster (200 images)\n",
        "...\n",
        "```\n",
        "\n",
        "If you found out you data is relative high distributed between classes (the bad example). you may reduce the images of several classes, or scale up all the classes images number into the same number like the below python code."
      ],
      "metadata": {
        "id": "vcp2SoyXbQwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle high distributed classes, if no just skip this section\n",
        "# Or you may see see how to handle the situations\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "def genReport(path):\n",
        "    \"\"\"\n",
        "    Generate report regarding the folder informations\n",
        "    \"\"\"\n",
        "    totalcount = 0\n",
        "    tinydict = {}\n",
        "\n",
        "    for ind, folder in enumerate( os.listdir(path) ) :\n",
        "\n",
        "        insidePath = os.path.join(path, folder)\n",
        "        tinydict[folder] = len( os.listdir(insidePath) )\n",
        "        totalcount += len( os.listdir(insidePath) )\n",
        "\n",
        "    numOnly = np.array( [ value for key, value in tinydict.items() ] )\n",
        "\n",
        "    avgVal = int(np.average(numOnly))\n",
        "    meanVal =  int(np.median(numOnly))\n",
        "    maxVal = max(tinydict.items(), key = lambda k : k[1])\n",
        "    minVal = min(tinydict.items(), key = lambda k : k[1])\n",
        "\n",
        "    print(\"-------------------------------\")\n",
        "    print(\"Generated report for\" , path)\n",
        "    print(\"Total folder:\" , len(numOnly))\n",
        "    print(\"Total items:\" , totalcount)\n",
        "    print(\"----------------\")\n",
        "    print(\"Avg:\" , avgVal )\n",
        "    print(\"Mean:\" , meanVal )\n",
        "    print(\"Max:\" , maxVal )\n",
        "    print(\"Min:\" , minVal )\n",
        "    print(\"-------------------------------\")\n",
        "\n",
        "    return { \"minVal\":minVal[1], \"maxVal\":maxVal[1], \"avgVal\" : avgVal, \"meanVal\": meanVal }\n",
        "\n",
        "def scaleByMax(path, result, offset = 0):\n",
        "\n",
        "    \"\"\"\n",
        "    Scaling folders items helpers\n",
        "    \"\"\"\n",
        "\n",
        "    for ind, folder in enumerate( os.listdir(path) ):\n",
        "        insidePath = os.path.join(path, folder)\n",
        "        insidePathArr = os.listdir(insidePath)\n",
        "\n",
        "        copyVal = result[\"maxVal\"] - len(insidePathArr) + offset\n",
        "\n",
        "        print(\"---------------\")\n",
        "        print(\"current:\", len(insidePathArr))\n",
        "        print(\"copy count:\" , copyVal)\n",
        "        print(\"---------------\")\n",
        "\n",
        "        for k in range(copyVal):\n",
        "            inside_folder = np.random.choice( insidePathArr )\n",
        "            shutil.copy( os.path.join(insidePath, inside_folder) , os.path.join(insidePath, \"newImg\" + str(k) + inside_folder ))\n",
        "\n",
        "def scaleDataSmart(dataPath, offset):\n",
        "    \"\"\"\n",
        "    Scale your data by the max numbers of items occur in all files\n",
        "    \"\"\"\n",
        "\n",
        "    scaleByMax( dataPath, genReport(dataPath) , offset )\n",
        "    print(\"After Report:\")\n",
        "    genReport(dataPath)\n",
        "\n",
        "# the folder structure is same as top mentioned one\n",
        "if __name__ == \"__main__\":\n",
        "  scaleDataSmart(\"yourDataFolderPath\", 50)"
      ],
      "metadata": {
        "id": "D56pzeFagLIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q&A\n",
        "\n",
        "Q.   \n",
        "Is the images resolutions matters to the training?  \n",
        "A.     \n",
        "Ususlly the model will resize your input images from original size to a relative small size (e.g. 224 x 224, 300x300, 480 x 480 ...)   \n",
        "Depends on the requirement of the models, the original images resolutions are not really matters to final result unless the images are really small (like 50 x 50). In general bigger than the model requirement is ok.\n"
      ],
      "metadata": {
        "id": "BDvmH1lJeAms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow keras provided a great utilis to create a tfDataset\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory  \n",
        "\n",
        "No need to run the below code, just a example of how we use this function to import data from out devices.\n"
      ],
      "metadata": {
        "id": "yLpH5rWCdSdw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbdgOOa6bQWJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "train_data_dir = \"your training dataset\"\n",
        "batch_size = 258\n",
        "imgSize = 300\n",
        "dataSeed = 1337\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  train_data_dir, seed=dataSeed, \n",
        "  image_size=(imgSize, imgSize), color_mode='rgb'\n",
        ")"
      ]
    }
  ]
}